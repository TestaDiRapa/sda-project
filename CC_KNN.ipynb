{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel K-NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MYIwcDnRgdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f4dc6dae-521f-4d29-8fb1-ddb72a890f56"
      },
      "source": [
        "# GETTING SPARK IN COLAB: https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb#scrollTo=9_Uz1NL4gHFx\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-29 14:06:21--  https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 2a01:4f8:10a:201a::2\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 220272364 (210M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.0.0-bin-hadoop2.7.tgz.1’\n",
            "\n",
            "spark-3.0.0-bin-had 100%[===================>] 210.07M  24.6MB/s    in 9.2s    \n",
            "\n",
            "2020-06-29 14:06:30 (22.7 MB/s) - ‘spark-3.0.0-bin-hadoop2.7.tgz.1’ saved [220272364/220272364]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yReeGOPlRiaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INIT STUFF OF ANY SPARK CODE\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark \n",
        "conf = pyspark.SparkConf().setMaster('local[4]').setAppName('my first Spark')\n",
        "sc = pyspark.SparkContext( conf=conf )\n",
        "\n",
        "# OTHER STUFF\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlNtDciqSVcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4c752740-7cae-45f9-9058-07c8fde8c7ec"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import preprocessing\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data[:,:2], iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "D_train = np.column_stack((X_train, y_train))\n",
        "\n",
        "RDD = sc.parallelize(D_train).cache()\n",
        "k = 5\n",
        "k_br = sc.broadcast(k)\n",
        "\n",
        "def euclidean_distance(element):\n",
        "  return ((element[:-1]-x_br.value)**2).sum()\n",
        "\n",
        "def k_nearest(data):\n",
        "  return sorted(data, key=euclidean_distance)[:k]\n",
        "\n",
        "p_labels = []\n",
        "for x in X_test:\n",
        "  x_br = sc.broadcast(x)\n",
        "  nn = k_nearest(RDD.mapPartitions(distances).collect())\n",
        "  counter = dict()\n",
        "  for n in nn:\n",
        "    d_k = int(n[-1])\n",
        "    if d_k in counter:\n",
        "      counter[d_k] += 1\n",
        "    else:\n",
        "      counter[d_k] = 1\n",
        "  p_labels.append(max(counter.keys(), key=lambda k: counter[k]))\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=k, algorithm=\"brute\")\n",
        "neigh.fit(X_train, y_train)\n",
        "s_labels = []\n",
        "for x in X_test:\n",
        "  s_labels.append(neigh.predict([x])[0])\n",
        "\n",
        "perc = 0\n",
        "for x, y in zip(p_labels, s_labels):\n",
        "  if x == y:\n",
        "    perc += 1\n",
        "print(\"Reality is:\\t\", list(y_test))\n",
        "print(\"Sklearn says:\\t\", s_labels)\n",
        "print(\"And I say:\\t\", p_labels)\n",
        "print(\"{:.2f}% match\".format(perc*100/len(p_labels)))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reality is:\t [1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
            "Sklearn says:\t [1, 0, 2, 1, 1, 0, 1, 2, 1, 2, 2, 0, 0, 0, 0, 2, 2, 1, 2, 2, 0, 1, 0, 2, 2, 1, 1, 2, 0, 0]\n",
            "And I say:\t [1, 0, 2, 1, 1, 0, 1, 2, 1, 2, 2, 0, 0, 0, 0, 2, 2, 1, 2, 2, 0, 1, 0, 2, 2, 1, 1, 2, 0, 0]\n",
            "100.00% match\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}